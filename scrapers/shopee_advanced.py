"""
Advanced Shopee scraper with real data extraction capabilities
ì‹¤ì œ Shopee ë°ì´í„°ë¥¼ ì¶”ì¶œí•˜ëŠ” ê³ ê¸‰ ìŠ¤í¬ë˜í¼
"""

import time
import random
import logging
import json
import re
from typing import List, Dict, Any, Optional
from datetime import datetime
from pathlib import Path
import sys

try:
    from selenium import webdriver
    from selenium.webdriver.chrome.options import Options
    from selenium.webdriver.chrome.service import Service
    from selenium.webdriver.common.by import By
    from selenium.webdriver.support.ui import WebDriverWait
    from selenium.webdriver.support import expected_conditions as EC
    from selenium.common.exceptions import TimeoutException, NoSuchElementException
    from webdriver_manager.chrome import ChromeDriverManager
    import undetected_chromedriver as uc
except ImportError as e:
    print(f"Selenium import error: {e}")

import requests
from bs4 import BeautifulSoup
from fake_useragent import UserAgent
from urllib.parse import quote, urljoin

# Add the project root to Python path for imports
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

# Load environment variables
from dotenv import load_dotenv
load_dotenv()

# Import SupabaseClient
try:
    from database.supabase_client import SupabaseClient
except ImportError:
    SupabaseClient = None
    print("âš ï¸ SupabaseClient not available - data will not be saved to database")

# from utils.anti_bot_system import AntiBotSystem
# from utils.ethical_scraping import ScrapingPolicy

logger = logging.getLogger(__name__)


class AdvancedShopeeScraper:
    """ê³ ê¸‰ Shopee ìŠ¤í¬ë˜í¼ - ì‹¤ì œ ë°ì´í„° ì¶”ì¶œ"""
    
    def __init__(
        self,
        anti_bot_system = None,
        scraping_policy = None,
        base_url: str = "https://shopee.ph",
        use_undetected: bool = True
    ):
        self.base_url = base_url
        self.use_undetected = use_undetected
        self.driver = None
        self.user_agent = UserAgent()
        self.collection_date = datetime.now()
        self.wait_timeout = 30
        
        # Initialize anti-bot system
        self.anti_bot_system = anti_bot_system
        if not self.anti_bot_system:
            # Create a simple mock system
            class MockAntiBot:
                def apply_delay(self):
                    time.sleep(random.uniform(2, 5))
            self.anti_bot_system = MockAntiBot()
        
        # Initialize scraping policy
        self.scraping_policy = scraping_policy
        if not self.scraping_policy:
            class MockPolicy:
                def wait_for_rate_limit(self):
                    time.sleep(1)
            self.scraping_policy = MockPolicy()
        
        # Initialize Supabase client if available
        self.supabase_client = None
        if SupabaseClient:
            try:
                self.supabase_client = SupabaseClient()
                logger.info("âœ… Supabase client initialized")
            except Exception as e:
                logger.warning(f"âš ï¸ Failed to initialize Supabase client: {e}")
                self.supabase_client = None
    
    def _setup_advanced_driver(self):
        """ê³ ê¸‰ í¬ë¡¬ ë“œë¼ì´ë²„ ì„¤ì • (undetected-chromedriver ì‚¬ìš©)"""
        try:
            if self.use_undetected:
                # undetected-chromedriver ì‚¬ìš© (ê°„ì†Œí™”ëœ ì˜µì…˜)
                options = uc.ChromeOptions()
                options.add_argument('--headless=new')
                options.add_argument('--no-sandbox')
                options.add_argument('--disable-dev-shm-usage')
                options.add_argument('--window-size=1366,768')
                
                # User-Agent ì„¤ì •
                user_agent = self.user_agent.random
                options.add_argument(f'--user-agent={user_agent}')
                
                self.driver = uc.Chrome(options=options, version_main=None)
                
            else:
                # ì¼ë°˜ Selenium ì‚¬ìš©
                options = Options()
                options.add_argument('--headless=new')
                options.add_argument('--no-sandbox')
                options.add_argument('--disable-dev-shm-usage')
                options.add_argument('--disable-blink-features=AutomationControlled')
                options.add_experimental_option("excludeSwitches", ["enable-automation"])
                options.add_experimental_option('useAutomationExtension', False)
                options.add_argument(f'--user-agent={self.user_agent.random}')
                
                service = Service(ChromeDriverManager().install())
                self.driver = webdriver.Chrome(service=service, options=options)
            
            # ìë™í™” ê°ì§€ ë°©ì§€ ìŠ¤í¬ë¦½íŠ¸
            self.driver.execute_script("Object.defineProperty(navigator, 'webdriver', {get: () => undefined})")
            self.driver.set_page_load_timeout(60)
            
            logger.info("âœ… Advanced WebDriver setup completed")
            
        except Exception as e:
            logger.error(f"âŒ Failed to setup advanced WebDriver: {e}")
            raise
    
    def _wait_and_scroll(self, wait_time: int = 10):
        """í˜ì´ì§€ ë¡œë“œ ëŒ€ê¸° ë° ìŠ¤í¬ë¡¤ë§"""
        try:
            # ì´ˆê¸° ëŒ€ê¸°
            time.sleep(wait_time)
            
            # í˜ì´ì§€ ì™„ì „ ë¡œë“œ í™•ì¸
            WebDriverWait(self.driver, 30).until(
                lambda d: d.execute_script("return document.readyState") == "complete"
            )
            
            # ì ì§„ì  ìŠ¤í¬ë¡¤ë§ìœ¼ë¡œ ë™ì  ì½˜í…ì¸  ë¡œë“œ
            total_height = self.driver.execute_script("return document.body.scrollHeight")
            current_position = 0
            scroll_increment = 500
            
            while current_position < total_height:
                # ìŠ¤í¬ë¡¤ ë‹¤ìš´
                self.driver.execute_script(f"window.scrollTo(0, {current_position});")
                time.sleep(random.uniform(1, 3))
                current_position += scroll_increment
                
                # í˜ì´ì§€ ë†’ì´ ì¬í™•ì¸ (ë™ì  ë¡œë”©ìœ¼ë¡œ ì¸í•œ ë³€í™”)
                new_height = self.driver.execute_script("return document.body.scrollHeight")
                if new_height > total_height:
                    total_height = new_height
            
            # ë§¨ ìœ„ë¡œ ìŠ¤í¬ë¡¤
            self.driver.execute_script("window.scrollTo(0, 0);")
            time.sleep(2)
            
            logger.info("âœ… Page scrolling and loading completed")
            
        except Exception as e:
            logger.warning(f"âš ï¸ Scroll and wait warning: {e}")
    
    def _extract_product_data(self, product_elements: list) -> List[Dict[str, Any]]:
        """ì œí’ˆ ìš”ì†Œì—ì„œ ì‹¤ì œ ë°ì´í„° ì¶”ì¶œ"""
        products = []
        
        for i, element in enumerate(product_elements):
            try:
                product_data = {
                    'collection_date': self.collection_date.isoformat(),
                    'search_keyword': '',
                    'product_name': 'Unknown Product',
                    'product_url': '',
                    'price': '',
                    'seller_name': 'Unknown Seller',
                    'rating': None,
                    'review_count': None,
                    'image_url': '',
                    'discount_info': {}
                }
                
                # ì œí’ˆëª… ì¶”ì¶œ (ì—¬ëŸ¬ ì„ íƒì ì‹œë„)
                name_selectors = [
                    '[data-sqe="name"]',
                    '.shopee-search-item-result__text',
                    'a[data-sqe="link"] span',
                    '.item-name',
                    '[title]'
                ]
                
                for selector in name_selectors:
                    try:
                        name_element = element.find_element(By.CSS_SELECTOR, selector)
                        name = name_element.get_attribute('title') or name_element.text
                        if name and name.strip():
                            product_data['product_name'] = name.strip()
                            break
                    except:
                        continue
                
                # ê°€ê²© ì¶”ì¶œ
                price_selectors = [
                    '.shopee-price',
                    '[data-sqe="price"]',
                    '.price',
                    '.item-price'
                ]
                
                for selector in price_selectors:
                    try:
                        price_element = element.find_element(By.CSS_SELECTOR, selector)
                        price_text = price_element.text
                        if price_text and 'â‚±' in price_text:
                            product_data['price'] = price_text.strip()
                            break
                    except:
                        continue
                
                # URL ì¶”ì¶œ
                try:
                    link_element = element.find_element(By.TAG_NAME, 'a')
                    href = link_element.get_attribute('href')
                    if href:
                        product_data['product_url'] = href
                except:
                    pass
                
                # ì´ë¯¸ì§€ URL ì¶”ì¶œ
                try:
                    img_element = element.find_element(By.TAG_NAME, 'img')
                    img_src = img_element.get_attribute('src')
                    if img_src:
                        product_data['image_url'] = img_src
                except:
                    pass
                
                # í‰ì  ì¶”ì¶œ
                rating_selectors = [
                    '.shopee-rating',
                    '[data-sqe="rating"]',
                    '.rating'
                ]
                
                for selector in rating_selectors:
                    try:
                        rating_element = element.find_element(By.CSS_SELECTOR, selector)
                        rating_text = rating_element.text
                        # ìˆ«ìë§Œ ì¶”ì¶œ
                        rating_match = re.search(r'(\d+\.?\d*)', rating_text)
                        if rating_match:
                            product_data['rating'] = float(rating_match.group(1))
                            break
                    except:
                        continue
                
                # ìœ íš¨í•œ ì œí’ˆì¸ì§€ í™•ì¸ (ì œí’ˆëª…ì´ ìˆê³  URLì´ ìˆëŠ” ê²½ìš°)
                if (product_data['product_name'] != 'Unknown Product' and 
                    product_data['product_url'] and 
                    'shopee.ph' in product_data['product_url']):
                    products.append(product_data)
                    logger.debug(f"âœ… Extracted product: {product_data['product_name'][:50]}...")
                
            except Exception as e:
                logger.debug(f"âš ï¸ Error extracting product {i}: {e}")
                continue
        
        return products
    
    def search_products_advanced(self, keyword: str, limit: int = 20) -> List[Dict[str, Any]]:
        """ê³ ê¸‰ ì œí’ˆ ê²€ìƒ‰ - ì‹¤ì œ ë°ì´í„° ì¶”ì¶œ"""
        try:
            if not self.driver:
                self._setup_advanced_driver()
            
            # ê²€ìƒ‰ URL êµ¬ì„±
            search_url = f"{self.base_url}/search?keyword={quote(keyword)}&sortBy=sales"
            
            logger.info(f"ğŸ” Advanced search for: {keyword}")
            logger.info(f"ğŸ“ Navigating to: {search_url}")
            
            # í˜ì´ì§€ ë¡œë“œ
            self.driver.get(search_url)
            
            # í˜ì´ì§€ ë¡œë“œ ë° ìŠ¤í¬ë¡¤ ëŒ€ê¸°
            self._wait_and_scroll(15)
            
            # ì œí’ˆ ìš”ì†Œ ì°¾ê¸° (ì—¬ëŸ¬ ì„ íƒì ì‹œë„)
            product_selectors = [
                '[data-sqe="item"]',
                '.shopee-search-item-result__item',
                '.col-xs-2-4.shopee-search-item-result__item',
                '.item-card-wrapper',
                'div[data-sqe="item"]',
                'a[data-sqe="link"]'
            ]
            
            products = []
            found_elements = []
            
            for selector in product_selectors:
                try:
                    elements = self.driver.find_elements(By.CSS_SELECTOR, selector)
                    if len(elements) >= 5:  # ì¶©ë¶„í•œ ìš”ì†Œë¥¼ ì°¾ì•˜ì„ ë•Œ
                        found_elements = elements
                        logger.info(f"âœ… Found {len(elements)} product elements using: {selector}")
                        break
                except Exception as e:
                    logger.debug(f"Selector {selector} failed: {e}")
                    continue
            
            if not found_elements:
                logger.warning("âŒ No product elements found with any selector")
                return []
            
            # ì œí’ˆ ë°ì´í„° ì¶”ì¶œ
            products = self._extract_product_data(found_elements[:limit])
            
            # ê²€ìƒ‰ í‚¤ì›Œë“œ ì„¤ì •
            for product in products:
                product['search_keyword'] = keyword
            
            logger.info(f"âœ… Successfully extracted {len(products)} real products for '{keyword}'")
            return products
            
        except Exception as e:
            logger.error(f"âŒ Error in advanced product search: {e}")
            return []
    
    def get_flash_deal_products_advanced(self, limit: int = 20, save_to_db: bool = True) -> List[Dict[str, Any]]:
        """ê³ ê¸‰ í”Œë˜ì‹œ ë”œ ì œí’ˆ ìˆ˜ì§‘"""
        try:
            logger.info("ğŸ”¥ Advanced Flash Deal collection starting...")
            
            # í”Œë˜ì‹œ ë”œ ê´€ë ¨ í‚¤ì›Œë“œ
            flash_keywords = [
                "sale",
                "discount", 
                "promo",
                "deal",
                "clearance"
            ]
            
            all_products = []
            products_per_keyword = max(1, limit // len(flash_keywords))
            
            for keyword in flash_keywords:
                try:
                    logger.info(f"ğŸ” Searching flash deals: {keyword}")
                    products = self.search_products_advanced(keyword, limit=products_per_keyword)
                    
                    # í”Œë˜ì‹œ ë”œ ë§ˆí‚¹
                    for product in products:
                        product['search_keyword'] = f"flash_deal_{keyword}"
                        product['product_type'] = 'flash_deal'
                        
                    all_products.extend(products)
                    
                    # í‚¤ì›Œë“œ ê°„ ëŒ€ê¸°
                    self.anti_bot_system.apply_delay()
                    
                except Exception as e:
                    logger.warning(f"âš ï¸ Error with flash keyword '{keyword}': {e}")
                    continue
            
            # ì¤‘ë³µ ì œê±°
            seen_urls = set()
            unique_products = []
            for product in all_products:
                url = product.get('product_url', '')
                if url and url not in seen_urls:
                    seen_urls.add(url)
                    unique_products.append(product)
            
            final_products = unique_products[:limit]
            logger.info(f"âœ… Collected {len(final_products)} unique flash deal products")
            
            # ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥
            if save_to_db and final_products:
                self._save_to_supabase(final_products)
            
            return final_products
            
        except Exception as e:
            logger.error(f"âŒ Error collecting advanced flash deals: {e}")
            return []
    
    def get_trending_products_advanced(self, categories: List[str] = None, limit: int = 20, save_to_db: bool = True) -> List[Dict[str, Any]]:
        """ê³ ê¸‰ íŠ¸ë Œë”© ì œí’ˆ ìˆ˜ì§‘"""
        try:
            logger.info("ğŸ“ˆ Advanced Trending Products collection starting...")
            
            if not categories:
                categories = [
                    "beauty",
                    "fashion", 
                    "electronics",
                    "skincare",
                    "phone"
                ]
            
            all_products = []
            products_per_category = max(1, limit // len(categories))
            
            for category in categories:
                try:
                    logger.info(f"ğŸ·ï¸ Searching trending: {category}")
                    products = self.search_products_advanced(category, limit=products_per_category)
                    
                    # íŠ¸ë Œë”© ë§ˆí‚¹
                    for product in products:
                        product['search_keyword'] = f"trending_{category}"
                        product['product_type'] = 'trending'
                        product['category'] = category
                        
                    all_products.extend(products)
                    
                    # ì¹´í…Œê³ ë¦¬ ê°„ ëŒ€ê¸°
                    self.anti_bot_system.apply_delay()
                    
                except Exception as e:
                    logger.warning(f"âš ï¸ Error with category '{category}': {e}")
                    continue
            
            # ì¤‘ë³µ ì œê±°
            seen_urls = set()
            unique_products = []
            for product in all_products:
                url = product.get('product_url', '')
                if url and url not in seen_urls:
                    seen_urls.add(url)
                    unique_products.append(product)
            
            final_products = unique_products[:limit]
            logger.info(f"âœ… Collected {len(final_products)} unique trending products")
            
            # ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥
            if save_to_db and final_products:
                self._save_to_supabase(final_products)
            
            return final_products
            
        except Exception as e:
            logger.error(f"âŒ Error collecting advanced trending products: {e}")
            return []
    
    def _save_to_supabase(self, products: List[Dict[str, Any]]) -> bool:
        """Supabaseì— ì œí’ˆ ì €ì¥"""
        if not self.supabase_client:
            logger.warning("âš ï¸ Supabase client not available - skipping database save")
            return False
        
        if not products:
            logger.info("â„¹ï¸ No products to save to database")
            return True
        
        try:
            logger.info(f"ğŸ’¾ Saving {len(products)} real products to Supabase...")
            
            # ë°ì´í„° í¬ë§·íŒ…
            formatted_products = []
            for product in products:
                formatted_product = {
                    'collection_date': product.get('collection_date', self.collection_date.isoformat()),
                    'search_keyword': product.get('search_keyword', 'unknown'),
                    'product_name': product.get('product_name', 'Unknown Product'),
                    'seller_name': product.get('seller_name', 'Unknown Seller'),
                    'price': self._extract_price_value(product.get('price', '0')),
                    'currency': 'PHP',
                    'rating': product.get('rating'),
                    'review_count': product.get('review_count'),
                    'product_url': product.get('product_url'),
                    'image_url': product.get('image_url'),
                    'category': product.get('category', product.get('product_type', 'general')),
                    'discount_info': {
                        'product_type': product.get('product_type'),
                        'original_keyword': product.get('search_keyword'),
                        'scrape_method': 'advanced_shopee_scraper',
                        'is_real_data': True  # ì‹¤ì œ ë°ì´í„°ì„ì„ í‘œì‹œ
                    }
                }
                formatted_products.append(formatted_product)
            
            # ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥
            success = self.supabase_client.insert_shopee_products(formatted_products)
            
            if success:
                logger.info(f"âœ… Successfully saved {len(formatted_products)} real products to Supabase")
                return True
            else:
                logger.error("âŒ Failed to save products to Supabase")
                return False
                
        except Exception as e:
            logger.error(f"âŒ Error saving to Supabase: {e}")
            return False
    
    def _extract_price_value(self, price_str: str) -> Optional[float]:
        """ê°€ê²© ë¬¸ìì—´ì—ì„œ ìˆ«ì ê°’ ì¶”ì¶œ"""
        try:
            if not price_str or price_str == 'N/A':
                return None
            # í†µí™” ê¸°í˜¸ì™€ ì‰¼í‘œ ì œê±°, ìˆ«ìë§Œ ì¶”ì¶œ
            import re
            price_match = re.search(r'[\d,]+\.?\d*', str(price_str).replace(',', ''))
            if price_match:
                return float(price_match.group())
            return None
        except:
            return None
    
    def close(self):
        """ë¸Œë¼ìš°ì € ì¢…ë£Œ"""
        if self.driver:
            self.driver.quit()
            self.driver = None
            logger.info("âœ… Browser closed")


def main():
    """í…ŒìŠ¤íŠ¸ í•¨ìˆ˜"""
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )
    
    scraper = AdvancedShopeeScraper(use_undetected=True)
    
    try:
        print("ğŸ›’ Testing Advanced Shopee scraper...")
        
        # í…ŒìŠ¤íŠ¸ ê²€ìƒ‰
        test_keyword = "skincare"
        products = scraper.search_products_advanced(test_keyword, limit=5)
        
        print(f"âœ… Found {len(products)} real products for '{test_keyword}'")
        
        if products:
            print("\nğŸ“¦ Real products found:")
            for i, product in enumerate(products):
                print(f"{i+1}. {product['product_name'][:50]}... - {product['price']}")
                print(f"   URL: {product['product_url'][:80]}...")
                print()
        
        return products
        
    except Exception as e:
        print(f"âŒ Error testing scraper: {e}")
        return []
    
    finally:
        scraper.close()


if __name__ == "__main__":
    main()