"""
Main entry point for Vootcamp PH Data Scraper
"""

import argparse
import logging
import sys
import time
import traceback
from datetime import datetime
from typing import Dict, Any, List

# Load environment variables first
from dotenv import load_dotenv
load_dotenv()

from config.settings import Settings

# Import database client
from database.supabase_client import SupabaseClient

# Import core scrapers (focused on 4 key scrapers)
from scrapers.google_trends import GoogleTrendsScraper
from scrapers.lazada_persona_scraper import LazadaPersonaScraper  # Persona-targeted scraper
from scrapers.tiktok_shop_scraper import TikTokShopScraper
from scrapers.local_event_scraper import LocalEventScraper  # Local events scraper

# Import Event-Trend Correlation Engine v2.3
from pathlib import Path
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))
from engines.event_trend_analyzer import EventTrendAnalyzer

# Import utilities
from utils.anti_bot_system import AntiBotSystem
from utils.ethical_scraping import ScrapingPolicy

# Import persona recommendation engine
from persona_recommendation_engine import PersonaRecommendationEngine


def setup_logging():
    """Setup logging configuration"""
    # Create logs directory if it doesn't exist
    import os
    os.makedirs('logs', exist_ok=True)
    
    # Configure logging
    logging.basicConfig(
        level=getattr(logging, "INFO"),  # Use INFO level directly
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
        handlers=[
            logging.FileHandler(f"logs/scraper_{datetime.now().strftime('%Y%m%d')}.log"),
            logging.StreamHandler(sys.stdout)
        ]
    )
    
    logger = logging.getLogger('vootcamp_scraper')
    return logger


def initialize_components(logger):
    """Initialize core components (database, anti-bot, policy)"""
    try:
        # Initialize database client
        logger.info("Initializing Supabase database client...")
        database_client = SupabaseClient()
        logger.info("âœ… Database client initialized successfully")
        
        # Initialize anti-bot system
        logger.info("Initializing anti-bot protection system...")
        try:
            from utils.anti_bot import AntiBotConfig
            config = AntiBotConfig()  # Use default config
            anti_bot_system = AntiBotSystem(config)
            logger.info("âœ… Anti-bot system initialized")
        except Exception as e:
            logger.warning(f"âš ï¸ Advanced anti-bot system failed, using mock: {e}")
            # Create a simple mock for basic functionality
            from unittest.mock import Mock
            anti_bot_system = Mock()
            anti_bot_system.simulate_human_behavior = Mock()
            anti_bot_system.get_driver = Mock()
            logger.info("âœ… Mock anti-bot system initialized")
        
        # Initialize ethical scraping policy
        logger.info("Initializing ethical scraping policies...")
        scraping_policy = ScrapingPolicy()
        logger.info("âœ… Scraping policy initialized")
        
        return database_client, anti_bot_system, scraping_policy
        
    except Exception as e:
        logger.error(f"âŒ Failed to initialize components: {e}")
        logger.error(traceback.format_exc())
        raise


def run_google_trends_scraper(database_client, anti_bot_system, scraping_policy, logger) -> Dict[str, Any]:
    """Run Google Trends scraper"""
    scraper_name = "Google Trends"
    results = {"name": scraper_name, "success": False, "data_count": 0, "error": None, "duration": 0}
    
    start_time = time.time()
    logger.info(f"ğŸš€ Starting {scraper_name} scraper...")
    
    try:
        scraper = GoogleTrendsScraper(anti_bot_system, scraping_policy)
        
        # Sample keywords for testing (you can modify this)
        test_keywords = ["shopee", "skincare", "fashion", "food delivery", "travel"]
        
        logger.info(f"Collecting trends for keywords: {test_keywords}")
        trends_data = scraper.get_trends(test_keywords)
        
        if trends_data:
            # Store in database
            database_client.insert_google_trends(trends_data)
            results["success"] = True
            results["data_count"] = len(test_keywords)
            logger.info(f"âœ… {scraper_name} completed successfully. Collected data for {len(test_keywords)} keywords")
        else:
            logger.warning(f"âš ï¸ {scraper_name} returned no data")
            
    except Exception as e:
        results["error"] = str(e)
        logger.error(f"âŒ {scraper_name} failed: {e}")
        logger.error(traceback.format_exc())
    
    finally:
        results["duration"] = round(time.time() - start_time, 2)
        logger.info(f"â±ï¸ {scraper_name} execution time: {results['duration']} seconds")
    
    return results


# DEPRECATED: Shopee scraper - replaced by Lazada Persona for better results
def run_lazada_persona_scraper(database_client, anti_bot_system, scraping_policy, logger) -> Dict[str, Any]:
    """Run Persona-targeted Lazada scraper for young Filipina beauty enthusiasts"""
    scraper_name = "Lazada Philippines (Persona-Targeted)"
    results = {"name": scraper_name, "success": False, "data_count": 0, "error": None, "duration": 0}
    
    start_time = time.time()
    logger.info(f"ğŸ¯ Starting {scraper_name} scraper...")
    
    try:
        # í˜ë¥´ì†Œë‚˜ íƒ€ê²Ÿ ìŠ¤í¬ë˜í¼ ì´ˆê¸°í™” 
        scraper = LazadaPersonaScraper(persona_name="young_filipina", use_undetected=True)
        
        logger.info(f"ğŸ¯ Target Persona: {scraper.persona.name}")
        logger.info(f"ğŸ‘¥ Age Group: {scraper.persona.age_group.value}")
        logger.info(f"ğŸ’° Price Range: â‚±{scraper.persona_filters.get('price_ranges', [])[0][0] if scraper.persona_filters.get('price_ranges') else 100}-{scraper.persona_filters.get('max_price', 2000)}")
        
        # í˜ë¥´ì†Œë‚˜ íƒ€ê²Ÿ ì œí’ˆ ìˆ˜ì§‘
        all_products = scraper.get_persona_trending_products(limit=15, save_to_db=True)
        
        if all_products:
            # ì„±ê³¼ í†µê³„ ê³„ì‚°
            avg_score = sum(p.get('persona_score', 0) for p in all_products) / len(all_products)
            high_relevance_count = sum(1 for p in all_products if p.get('persona_score', 0) > 70)
            brand_matches = sum(1 for p in all_products if p.get('brand_bonus', False))
            
            results["success"] = True
            results["data_count"] = len(all_products)
            results["persona_stats"] = {
                "average_persona_score": round(avg_score, 1),
                "high_relevance_products": high_relevance_count,
                "brand_matched_products": brand_matches,
                "persona_name": scraper.persona.name
            }
            
            logger.info(f"âœ… {scraper_name} completed successfully. Collected {len(all_products)} persona-targeted products")
            logger.info(f"ğŸ“Š Persona Performance:")
            logger.info(f"   - Average relevance score: {avg_score:.1f}/100")
            logger.info(f"   - High relevance (>70): {high_relevance_count}/{len(all_products)}")
            logger.info(f"   - Brand matches: {brand_matches}/{len(all_products)}")
        else:
            logger.warning(f"âš ï¸ {scraper_name} returned no persona-targeted products")
            
    except Exception as e:
        results["error"] = str(e)
        logger.error(f"âŒ {scraper_name} failed: {e}")
        logger.error(traceback.format_exc())
    
    finally:
        # Ensure driver cleanup
        try:
            if hasattr(scraper, 'driver') and scraper.driver:
                scraper.driver.quit()
        except:
            pass
        
        results["duration"] = round(time.time() - start_time, 2)
        logger.info(f"â±ï¸ {scraper_name} execution time: {results['duration']} seconds")
    
    return results


# DEPRECATED: Basic TikTok scraper - replaced by TikTok Shop for commercial data
def run_tiktok_shop_scraper(database_client, anti_bot_system, scraping_policy, logger) -> Dict[str, Any]:
    """Run TikTok Shop scraper"""
    scraper_name = "TikTok Shop Philippines"
    results = {"name": scraper_name, "success": False, "data_count": 0, "error": None, "duration": 0}
    
    start_time = time.time()
    logger.info(f"ğŸ›ï¸ Starting {scraper_name} scraper...")
    
    try:
        scraper = TikTokShopScraper(use_undetected=True, headless=True)
        
        all_products = []
        
        # 1. Top Products ìˆ˜ì§‘
        logger.info("ğŸ¯ Collecting Top Products...")
        top_products = scraper.get_top_products(limit=10)
        all_products.extend(top_products)
        
        if top_products:
            logger.info(f"âœ… Collected {len(top_products)} top products")
        else:
            logger.warning("âš ï¸ No top products found")
        
        # Delay between collections
        time.sleep(5)
        
        # 2. Flash Sale Products ìˆ˜ì§‘
        logger.info("âš¡ Collecting Flash Sale Products...")
        flash_products = scraper.get_flash_sale_products(limit=8)
        all_products.extend(flash_products)
        
        if flash_products:
            logger.info(f"âœ… Collected {len(flash_products)} flash sale products")
        else:
            logger.warning("âš ï¸ No flash sale products found")
        
        # Delay between collections
        time.sleep(5)
        
        # 3. Category Products ìˆ˜ì§‘ (beauty category)
        logger.info("ğŸ’„ Collecting Beauty Category Products...")
        beauty_products = scraper.get_category_products("beauty", limit=7)
        all_products.extend(beauty_products)
        
        if beauty_products:
            logger.info(f"âœ… Collected {len(beauty_products)} beauty category products")
        else:
            logger.warning("âš ï¸ No beauty category products found")
        
        if all_products:
            # Store in database
            database_client.insert_tiktok_shop_products(all_products)
            results["success"] = True
            results["data_count"] = len(all_products)
            
            # Calculate performance stats
            price_products = [p for p in all_products if p.get('price_numeric')]
            avg_price = sum(p['price_numeric'] for p in price_products) / len(price_products) if price_products else 0
            
            results["shop_stats"] = {
                "top_products": len(top_products),
                "flash_products": len(flash_products), 
                "category_products": len(beauty_products),
                "avg_price_php": round(avg_price, 2) if avg_price else 0,
                "products_with_price": len(price_products)
            }
            
            logger.info(f"âœ… {scraper_name} completed successfully. Collected {len(all_products)} products")
            logger.info(f"ğŸ“Š TikTok Shop Performance:")
            logger.info(f"   - Top Products: {len(top_products)}")
            logger.info(f"   - Flash Sale: {len(flash_products)}")
            logger.info(f"   - Beauty Category: {len(beauty_products)}")
            logger.info(f"   - Average Price: â‚±{avg_price:.2f}" if avg_price else "   - Average Price: N/A")
        else:
            logger.warning(f"âš ï¸ {scraper_name} returned no products")
            results["error"] = "No products found"
            
    except Exception as e:
        results["error"] = str(e)
        logger.error(f"âŒ {scraper_name} failed: {e}")
        logger.error(traceback.format_exc())
    
    finally:
        # Ensure driver cleanup
        try:
            if 'scraper' in locals():
                scraper.close()
        except:
            pass
        
        results["duration"] = round(time.time() - start_time, 2)
        logger.info(f"â±ï¸ {scraper_name} execution time: {results['duration']} seconds")
    
    return results


def run_local_event_scraper(database_client, anti_bot_system, scraping_policy, logger) -> Dict[str, Any]:
    """Run Local Event scraper for Philippine lifestyle events"""
    scraper_name = "Local Events Philippines"
    results = {"name": scraper_name, "success": False, "data_count": 0, "error": None, "duration": 0}
    
    start_time = time.time()
    logger.info(f"ğŸª Starting {scraper_name} scraper...")
    
    try:
        scraper = LocalEventScraper()
        
        logger.info("ğŸ™ï¸ Collecting events from lifestyle media sources...")
        logger.info("ğŸ“° Sources: Nylon Manila, Spot.ph, When in Manila")
        
        # Collect all events from multiple sources
        all_events = scraper.get_all_events()
        
        if all_events:
            # Store in database
            database_client.insert_local_events(all_events)
            results["success"] = True
            results["data_count"] = len(all_events)
            
            # Calculate event statistics
            event_types = {}
            has_dates = 0
            has_locations = 0
            sources = {}
            
            for event in all_events:
                # Count event types
                event_type = event.get('event_type', 'unknown')
                event_types[event_type] = event_types.get(event_type, 0) + 1
                
                # Count events with dates and locations
                if event.get('event_dates'):
                    has_dates += 1
                if event.get('event_location'):
                    has_locations += 1
                
                # Count sources
                source = event.get('source_website', 'unknown')
                sources[source] = sources.get(source, 0) + 1
            
            results["event_stats"] = {
                "total_events": len(all_events),
                "event_types": event_types,
                "events_with_dates": has_dates,
                "events_with_locations": has_locations,
                "sources": sources,
                "data_completeness": round((has_dates + has_locations) / (len(all_events) * 2) * 100, 1)
            }
            
            logger.info(f"âœ… {scraper_name} completed successfully. Collected {len(all_events)} events")
            logger.info(f"ğŸ“Š Event Statistics:")
            logger.info(f"   - Event Types: {dict(list(event_types.items())[:3])}")
            logger.info(f"   - With Dates: {has_dates}/{len(all_events)}")
            logger.info(f"   - With Locations: {has_locations}/{len(all_events)}")
            logger.info(f"   - Sources: {list(sources.keys())}")
            logger.info(f"   - Data Completeness: {results['event_stats']['data_completeness']}%")
        else:
            logger.warning(f"âš ï¸ {scraper_name} returned no events")
            results["error"] = "No events found"
            
    except Exception as e:
        results["error"] = str(e)
        logger.error(f"âŒ {scraper_name} failed: {e}")
        logger.error(traceback.format_exc())
    
    finally:
        results["duration"] = round(time.time() - start_time, 2)
        logger.info(f"â±ï¸ {scraper_name} execution time: {results['duration']} seconds")
    
    return results


def run_event_trend_analyzer(database_client, logger) -> Dict[str, Any]:
    """Run Event-Trend Correlation Engine v2.3 to analyze collected events"""
    scraper_name = "Event-Trend Correlation Engine v2.3"
    results = {"name": scraper_name, "success": False, "data_count": 0, "error": None, "duration": 0}
    
    start_time = time.time()
    logger.info(f"ğŸ”— Starting {scraper_name}...")
    
    try:
        # Initialize EventTrendAnalyzer
        analyzer = EventTrendAnalyzer()
        
        logger.info("ğŸ¯ Analyzing collected events with Google Trends data...")
        logger.info("ğŸ” Correlating local events with search trends...")
        
        # Analyze all events in the database
        analyzed_events = analyzer.analyze_all_events()
        
        if analyzed_events:
            results["success"] = True
            results["data_count"] = len(analyzed_events)
            
            # Calculate analysis statistics
            trend_statuses = {}
            total_score = 0
            high_trend_events = 0
            events_with_queries = 0
            
            for event in analyzed_events:
                # Count trend statuses
                status = event.get('trend_status', 'Unknown')
                trend_statuses[status] = trend_statuses.get(status, 0) + 1
                
                # Calculate statistics
                score = event.get('trend_score', 0)
                total_score += score
                
                if score >= 60:  # High trend threshold
                    high_trend_events += 1
                
                if event.get('top_related_queries'):
                    events_with_queries += 1
            
            avg_score = total_score / len(analyzed_events) if analyzed_events else 0
            
            results["analysis_stats"] = {
                "total_analyzed": len(analyzed_events),
                "trend_statuses": trend_statuses,
                "average_trend_score": round(avg_score, 1),
                "high_trend_events": high_trend_events,
                "events_with_related_queries": events_with_queries,
                "analysis_completion_rate": round((len(analyzed_events) / max(len(analyzed_events), 1)) * 100, 1)
            }
            
            logger.info(f"âœ… {scraper_name} completed successfully. Analyzed {len(analyzed_events)} events")
            logger.info(f"ğŸ“Š Analysis Statistics:")
            logger.info(f"   - Average trend score: {avg_score:.1f}/100")
            logger.info(f"   - High trend events (>60): {high_trend_events}/{len(analyzed_events)}")
            logger.info(f"   - Events with related queries: {events_with_queries}/{len(analyzed_events)}")
            logger.info(f"   - Trend status distribution: {dict(list(trend_statuses.items())[:3])}")
            
            # Log top trending events
            top_events = sorted(analyzed_events, key=lambda x: x.get('trend_score', 0), reverse=True)[:3]
            if top_events:
                logger.info(f"ğŸ”¥ Top trending events:")
                for i, event in enumerate(top_events, 1):
                    event_name = event.get('event_name') or event.get('name') or event.get('title', 'Unknown')
                    score = event.get('trend_score', 0)
                    status = event.get('trend_status', 'Unknown')
                    logger.info(f"   {i}. {event_name[:40]}... (Score: {score}, Status: {status})")
        else:
            logger.warning(f"âš ï¸ {scraper_name} found no events to analyze")
            results["error"] = "No events found for analysis"
            
    except Exception as e:
        results["error"] = str(e)
        logger.error(f"âŒ {scraper_name} failed: {e}")
        logger.error(traceback.format_exc())
    
    finally:
        results["duration"] = round(time.time() - start_time, 2)
        logger.info(f"â±ï¸ {scraper_name} execution time: {results['duration']} seconds")
    
    return results


def generate_summary_report(all_results: List[Dict[str, Any]], logger):
    """Generate and log execution summary"""
    logger.info("=" * 60)
    logger.info("ğŸ“Š EXECUTION SUMMARY REPORT")
    logger.info("=" * 60)
    
    total_duration = sum(result["duration"] for result in all_results)
    successful_scrapers = [r for r in all_results if r["success"]]
    failed_scrapers = [r for r in all_results if not r["success"]]
    total_data_collected = sum(result["data_count"] for result in all_results)
    
    logger.info(f"â±ï¸ Total execution time: {total_duration:.2f} seconds")
    logger.info(f"âœ… Successful scrapers: {len(successful_scrapers)}/{len(all_results)}")
    logger.info(f"ğŸ“Š Total data points collected: {total_data_collected}")
    
    # Individual scraper results
    for result in all_results:
        status = "âœ… SUCCESS" if result["success"] else "âŒ FAILED"
        logger.info(f"{status} | {result['name']}: {result['data_count']} items in {result['duration']}s")
        if result["error"]:
            logger.info(f"  â””â”€ Error: {result['error']}")
    
    # Recommendations
    if failed_scrapers:
        logger.warning("ğŸ”§ RECOMMENDATIONS:")
        for failed in failed_scrapers:
            if "Not implemented" in str(failed["error"]):
                logger.warning(f"  â€¢ Complete implementation of {failed['name']} scraper")
            else:
                logger.warning(f"  â€¢ Debug and fix {failed['name']}: {failed['error']}")
    
    logger.info("=" * 60)


def parse_arguments():
    """Parse command line arguments"""
    parser = argparse.ArgumentParser(
        description="Vootcamp PH Data Scraper with Persona Recommendation Engine",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  python main.py                    # Run normal scraping pipeline
  python main.py --debug            # Run with transparency report (debug mode)
  python main.py --persona-only     # Run only persona recommendation engine
  python main.py --debug --persona-only  # Run persona engine with debug output
        """
    )
    
    parser.add_argument(
        '--debug', 
        action='store_true',
        help='Enable debug mode with transparency report showing detailed scoring'
    )
    
    parser.add_argument(
        '--persona-only',
        action='store_true', 
        help='Run only the persona recommendation engine (skip scrapers)'
    )
    
    return parser.parse_args()

def run_persona_recommendation_engine(debug_mode: bool = False, logger=None) -> Dict[str, Any]:
    """Run Persona Recommendation Engine with optional debug mode"""
    engine_name = "Persona Recommendation Engine"
    results = {"name": engine_name, "success": False, "data_count": 0, "error": None, "duration": 0}
    
    start_time = time.time()
    if logger:
        logger.info(f"ğŸ¯ Starting {engine_name}...")
        if debug_mode:
            logger.info("ğŸ” Debug mode enabled - transparency report will be generated")
    
    try:
        engine = PersonaRecommendationEngine(debug_mode=debug_mode)
        
        if debug_mode:
            print("\n" + "="*80)
            print("ğŸ” PERSONA RECOMMENDATION ENGINE - TRANSPARENCY REPORT")
            print("="*80)
        
        # Generate recommendations for all personas
        full_report = engine.generate_full_recommendation_report()
        
        # Count total recommendations generated
        total_recommendations = 0
        total_content_ideas = 0
        
        for persona_data in full_report.get("personas", {}).values():
            total_recommendations += len(persona_data.get("product_recommendations", []))
            total_content_ideas += len(persona_data.get("content_ideas", []))
        
        results["success"] = True
        results["data_count"] = total_recommendations + total_content_ideas
        results["persona_stats"] = {
            "total_personas": len(full_report.get("personas", {})),
            "total_product_recommendations": total_recommendations,
            "total_content_ideas": total_content_ideas,
            "debug_mode": debug_mode
        }
        
        if logger:
            logger.info(f"âœ… {engine_name} completed successfully")
            logger.info(f"ğŸ“Š Generated {total_recommendations} product recommendations")
            logger.info(f"ğŸ’¡ Generated {total_content_ideas} content ideas")
            logger.info(f"ğŸ‘¥ Analyzed {len(full_report.get('personas', {}))} personas")
            
            if debug_mode:
                logger.info("ğŸ” Debug transparency report displayed above")
        
        # Save report to file
        import json
        filename = f"persona_recommendations_{'debug_' if debug_mode else ''}{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(full_report, f, ensure_ascii=False, indent=2)
        
        if logger:
            logger.info(f"ğŸ“ Report saved to: {filename}")
            
    except Exception as e:
        results["error"] = str(e)
        if logger:
            logger.error(f"âŒ {engine_name} failed: {e}")
            logger.error(traceback.format_exc())
    
    finally:
        results["duration"] = round(time.time() - start_time, 2)
        if logger:
            logger.info(f"â±ï¸ {engine_name} execution time: {results['duration']} seconds")
    
    return results

def main():
    """Main orchestration function"""
    args = parse_arguments()
    
    logger = setup_logging()
    logger.info("=" * 60)
    
    if args.persona_only:
        logger.info("ğŸ¯ VOOTCAMP PH PERSONA RECOMMENDATION ENGINE - STARTING")
        if args.debug:
            logger.info("ğŸ” DEBUG MODE - Transparency Report Enabled")
    else:
        logger.info("ğŸ‡µğŸ‡­ VOOTCAMP PH CORE 5 SCRAPERS + EVENT-TREND ANALYZER - STARTING")
        logger.info("ğŸ“Š Focus: Google Trends + Lazada Persona + TikTok Shop + Local Events + Event-Trend Analysis")
        if args.debug:
            logger.info("ğŸ” DEBUG MODE - Transparency Report Enabled")
    
    logger.info("=" * 60)
    
    all_results = []
    
    try:
        # If only running persona engine
        if args.persona_only:
            logger.info("ğŸ¯ Running Persona Recommendation Engine only...")
            persona_results = run_persona_recommendation_engine(debug_mode=args.debug, logger=logger)
            all_results.append(persona_results)
        
        else:
            # Validate settings
            logger.info("ğŸ”§ Validating configuration...")
            # Settings validation passed (using default settings)
            logger.info("âœ… Configuration validation passed")
            
            # Initialize core components
            database_client, anti_bot_system, scraping_policy = initialize_components(logger)
            
            # Run core 5 components (4 scrapers + 1 analyzer + persona engine)
            logger.info("ğŸ¯ Starting CORE 6 COMPONENTS execution sequence...")
            logger.info("ğŸ“Š Pipeline: Google Trends â†’ Lazada Persona â†’ TikTok Shop â†’ Local Events â†’ Event-Trend Analysis â†’ Persona Recommendations")
        
            # 1. Google Trends (Most stable - official API)
            logger.info("1ï¸âƒ£ Google Trends Philippines - Starting...")
            google_results = run_google_trends_scraper(database_client, anti_bot_system, scraping_policy, logger)
            all_results.append(google_results)
            
            # Delay between scrapers
            logger.info("â¸ï¸ Waiting 8 seconds before next scraper...")
            time.sleep(5)  # Optimized delay
            
            # 2. Lazada Persona (Real product data with persona targeting)
            logger.info("2ï¸âƒ£ Lazada Persona Targeting - Starting...")
            lazada_persona_results = run_lazada_persona_scraper(database_client, anti_bot_system, scraping_policy, logger)
            all_results.append(lazada_persona_results)
            
            # Delay before final scraper
            logger.info("â¸ï¸ Waiting 8 seconds before next scraper...")
            time.sleep(5)  # Optimized delay
            
            # 3. TikTok Shop (Latest trends and social commerce)
            logger.info("3ï¸âƒ£ TikTok Shop Philippines - Starting...")
            tiktok_shop_results = run_tiktok_shop_scraper(database_client, anti_bot_system, scraping_policy, logger)
            all_results.append(tiktok_shop_results)
            
            # Delay before final scraper
            logger.info("â¸ï¸ Waiting 5 seconds before next scraper...")
            time.sleep(3)  # Optimized delay
            
            # 4. Local Events (Experience-based content ideas)
            logger.info("4ï¸âƒ£ Local Events Philippines - Starting...")
            local_events_results = run_local_event_scraper(database_client, anti_bot_system, scraping_policy, logger)
            all_results.append(local_events_results)
            
            # Delay before event analysis
            logger.info("â¸ï¸ Waiting 3 seconds before trend analysis...")
            time.sleep(3)
        
            # 5. Event-Trend Correlation Engine v2.3 (Analyze collected events)
            logger.info("5ï¸âƒ£ Event-Trend Correlation Engine v2.3 - Starting...")
            event_trend_results = run_event_trend_analyzer(database_client, logger)
            all_results.append(event_trend_results)
            
            # Delay before persona engine
            logger.info("â¸ï¸ Waiting 3 seconds before persona recommendations...")
            time.sleep(3)
            
            # 6. Persona Recommendation Engine (Generate persona-based recommendations)
            logger.info("6ï¸âƒ£ Persona Recommendation Engine - Starting...")
            persona_results = run_persona_recommendation_engine(debug_mode=args.debug, logger=logger)
            all_results.append(persona_results)
        
        # Generate summary report
        generate_summary_report(all_results, logger)
        
        # Final status for all components
        successful_count = len([r for r in all_results if r["success"]])
        total_components = len(all_results)
        
        if args.persona_only:
            if successful_count == total_components:
                logger.info("ğŸ‰ PERSONA RECOMMENDATION ENGINE COMPLETED SUCCESSFULLY!")
                logger.info("âœ… Persona-based recommendations generated")
            else:
                logger.error("âŒ PERSONA RECOMMENDATION ENGINE FAILED")
                logger.error("ğŸ”§ Check configuration and system setup")
                sys.exit(1)
        else:
            if successful_count == total_components:
                logger.info("ğŸ‰ ALL CORE 6 COMPONENTS COMPLETED SUCCESSFULLY!")
                logger.info("âœ… Philippines market intelligence + event-trend analysis + persona recommendations complete")
            elif successful_count > 0:
                logger.info(f"âš ï¸ PARTIAL SUCCESS: {successful_count}/{total_components} components completed")
                logger.info("ğŸ“Š Some data collected - system partially operational")
            else:
                logger.error("âŒ ALL COMPONENTS FAILED")
                logger.error("ğŸ”§ Check network, credentials, and system configuration")
                sys.exit(1)
        
    except Exception as e:
        logger.error(f"ğŸ’¥ CRITICAL ERROR during execution: {e}")
        logger.error(traceback.format_exc())
        sys.exit(1)
    
    logger.info("=" * 60)
    if args.persona_only:
        logger.info("ğŸ VOOTCAMP PH PERSONA RECOMMENDATION ENGINE - COMPLETED")
        logger.info("ğŸ¯ Persona-based Recommendations Finished")
    else:
        logger.info("ğŸ VOOTCAMP PH CORE 6 COMPONENTS - COMPLETED")
        logger.info("ğŸ“Š Philippines Market Intelligence + Event-Trend Analysis + Persona Recommendations Finished")
    logger.info("=" * 60)


if __name__ == "__main__":
    main() 